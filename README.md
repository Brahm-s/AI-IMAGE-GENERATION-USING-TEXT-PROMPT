LoRA: Low-Rank Adaptation of Large Language Models (Supported by Pytorch only)

LoRA reduces the number of trainable parameters by learning pairs of rank-decompostion matrices while freezing the original weights. This vastly reduces the storage requirement for large language models adapted to specific tasks and enables efficient task-switching during deployment all without introducing inference latency. LoRA also outperforms several other adaptation methods including adapter, prefix-tuning, and fine-tuning.

Lora Paper - https://github.com/Brahm-s/AI-IMAGE-GENERATION-USING-TEXT-PROMPT/blob/cbdd62e5ff401222d37a6be5bb332318184ab070/Research_Paper.pdf


Credits - https://github.com/cloneofsimo/lora
